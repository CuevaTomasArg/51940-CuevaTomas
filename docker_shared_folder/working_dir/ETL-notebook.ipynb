{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871f5791-a3fc-4cbd-a563-e9bbec966221",
   "metadata": {},
   "source": [
    "# Segunda entrega - 51940 Cueva Tomas\n",
    "Este notebook contiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f06d1c-ceae-4031-b70d-059ae90a19c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.28.2)\n",
      "Requirement already satisfied: SQLAlchemy in /opt/conda/lib/python3.10/site-packages (2.0.10)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.10/site-packages (2.9.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy) (4.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy) (2.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas requests SQLAlchemy psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f1b9ad4-b00a-4ddc-aa9e-e3aaa050f9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import environ as env\n",
    "from pyspark.sql import SparkSession\n",
    "from etl import(\n",
    "    extract_CoinGecko_API as extract,\n",
    "    transform_CoinGecko_API as transform,\n",
    "    load_pyspark_redshift_connectr as load\n",
    ")\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3eaae7-35ba-4c6d-95d2-7f190007fc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/coder/working_dir/spark_drivers/postgresql-42.5.2.jar\n"
     ]
    }
   ],
   "source": [
    "extr = extract.Extract(\"https://api.coingecko.com/api/v3/\")\n",
    "transf = transform.Transform()\n",
    "ld = load.Load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fad5d-4e03-4e9b-a9d7-80c7fa067c03",
   "metadata": {},
   "source": [
    "### 100 criptos con mayor capitalización bursatil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfcf2fe4-0bf2-4c22-ae2a-62b0aad0db0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_top = extr.get_criptos_top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f30053d9-4434-4358-9844-70a677fb7d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_top = transf.transformation_top(json_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ebeb033-c195-497f-8a54-7de3e2012e43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 21 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   id                                100 non-null    object \n",
      " 1   symbol                            100 non-null    object \n",
      " 2   name                              100 non-null    object \n",
      " 3   current_price                     100 non-null    float64\n",
      " 4   market_cap                        100 non-null    int64  \n",
      " 5   market_cap_rank                   100 non-null    int64  \n",
      " 6   total_volume                      100 non-null    float64\n",
      " 7   high_24h                          100 non-null    float64\n",
      " 8   low_24h                           100 non-null    float64\n",
      " 9   price_change_24h                  100 non-null    float64\n",
      " 10  price_change_percentage_24h       100 non-null    float64\n",
      " 11  market_cap_change_24h             100 non-null    float64\n",
      " 12  market_cap_change_percentage_24h  100 non-null    float64\n",
      " 13  circulating_supply                100 non-null    float64\n",
      " 14  ath                               100 non-null    float64\n",
      " 15  ath_change_percentage             100 non-null    float64\n",
      " 16  ath_date                          100 non-null    object \n",
      " 17  atl                               100 non-null    float64\n",
      " 18  atl_change_percentage             100 non-null    float64\n",
      " 19  atl_date                          100 non-null    object \n",
      " 20  last_updated                      100 non-null    object \n",
      "dtypes: float64(13), int64(2), object(6)\n",
      "memory usage: 16.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_top.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f29b14cc-87f9-417b-b06c-86c2fd829055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = \"criptos_market_cap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c0e817e-dcae-4cb2-8af3-74e60a9657da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando ETL\n",
      "Convertir el DataFrame de pandas a un PySpark DataFrame\n",
      "DataFrame[id: string, symbol: string, name: string, current_price: double, market_cap: bigint, market_cap_rank: bigint, total_volume: double, high_24h: double, low_24h: double, price_change_24h: double, price_change_percentage_24h: double, market_cap_change_24h: double, market_cap_change_percentage_24h: double, circulating_supply: double, ath: double, ath_change_percentage: double, ath_date: string, atl: double, atl_change_percentage: double, atl_date: string, last_updated: string]\n",
      "Cargar el PySpark DataFrame en Redshift\n",
      "Se produjo excepción An error occurred while calling o98.save.\n",
      ": java.lang.ClassNotFoundException: com.amazon.redshift.jdbc.Driver\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:103)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:103)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ld.execute(df_top, table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
